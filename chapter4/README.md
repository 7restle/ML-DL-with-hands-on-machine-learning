# Chapter 4. Model training

## 4.1 Linear regression 선형 회귀
$$\hat{y}=\theta x$$

선을 이용하여 주어진 샘플의 결과값을 예측함
### 4.1.1 정규 방정식
$$ \hat{\theta}=(X^T X)^{-1} X^T y$$
### 4.1.2 계산 복잡도
$$O(n^2.4) ~ O(n^3)$$
## 4.2 경사 하강법 Gradiant Descent
여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘.  
비용함수의 최소화를 위해 반복해서 파라미터 조정, 현재 파라미터에 대해 그레디언트 계산 후 감소하는 방향으로 진행.  
결국 그레디언트가 0이 되는 점에서 최적화  
학습률이 너무 작으면 충분히 수렴하지 못하고, 너무 크면 발산해 최적점에 도달하지 못한다.  
함수에 지역 최솟값 (극소) 가 있으면 지역 최솟값에 빠질 수 있다.
### 4.2.1 배치 경사 하강법
전체 훈련 세트에 대해 반복, 따라서 훈련 세트가 클 수록 아주 느림
### 4.2.2 확률적 경사 하강법
랜덤한 한 개의 샘플을 선택하고 이를 통해 그레디언트를 계산함
### 4.2.3 미니 배치 경사 하강법
전체 훈련 세트를 작은 미니 배치로 나눈 후 임의의 한개의 미니 배치에 대해 그레디언트를 계산함
## 4.3 다항회귀
특성이 여러 개이거나 특성 사이의 관계를 찾을 수 있음.
## 4.4 학습 곡선
고차 다항 회귀가 될 수록 훈련 세트에서의 정확도를 올리기 위해 복잡해짐. 따라서 overfitting 발생
반대로 너무 작은 차수를 가져도 underfitting 발생. 이러한 정도를 확인하기 위해 learning curve 사용
## 4.5 규제가 있는 선형 모델
### 4.5.1 릿지 회귀
### 4.5.2 라쏘 회귀
### 4.5.3 엘라스틱넷 회귀
### 4.5.4 조기 종료
검증 오차가 최소에 도달하는 즉시 훈련을 멈추는 것, 과대적합을 방지할 수 있다.
## 4.6 로지스틱 회귀
회귀 알고리즘 중 일부는 분류에도 사용 가능. 로지스틱 회귀는 샘플이 특성 클래스에 속할 확률을 추정하는데 널리 사용됨.  
추정 확률이 임계값 보다 크면 1, 아니면 0을 예측하는 것이 이진 분류기
### 4.6.1 확률 추정
$$\hat{p}=h_\theta(x)=\sigma(\theta^Tx)$$
로지스틱은 0과 1 사이의 값을 출력하는 시그모이드 함수이다.
### 4.6.2 훈련과 비용 함수
$$c(\theta)=\begin{cases}
  -\log{(\hat{p})} & \text{if }y=1\\
  -\log{(1-\hat{p})} & \text{if }y=0
\end{cases}$$

로지스틱 회귀의 비용 함수는 로그 손실을 사용한다.
### 4.6.3 결정 경계
### 4.6.4 소프트맥스 회귀
